# 参考文献：

## 1、Automatically and Adaptively Identifying Severe Alerts for Online Service Systems

思路：构建一个在线服务的自动、自适应故障报警框架 AlertRank ，提取了一些列特征（报警短文本和警报时间周期特征、监控度量的单变量和多变量异常特征），采用XGBoost排序算法识别所有传入警报中的故障等级，再将【严重等级】报警优先提供给工程师。同时，支持外界输入来更新训练数据。其中，包含两个阶段：离线训练和在线排序。

挑战：标注开销、报警类型复杂【应用程序、网络、内存和中间件等】、报警因素多样【配置变更、软件升级等，如何自适应】、数据不平衡【最严重等级报警是最小部分的数据。如何从不平衡数据中学习】

数据集是 故障记录单ticket（非构化文本型、来自历史维护过程中严重故障和解决失败的故障）+ KPIs（metric） 

* 自动标记

二进制型标签：只与 故障记录单 相关，每个警报可被标记为【 严重 】或 【 非严重 】，作用是后期评估模型对故障等级判断的准确性。 

连续型标签（得分）：对于严重程度的打分（0-1）。采用 TF-IDF 矢量化 和 k-means 对 resolution record（故障记录单中的内容）进行聚类

* 文本特征抽取

1. 根据content, 进行分词，删除停用词和高频词

2. alert的描述是半结构化的数据，包含两类数据，一个常量字符串描述警告时间，一些参数记录系统变量。alert语法分析抽取常量字符串形成模板，模板中剩余的部分即变量值，值可以被忽略，因为会被server score反映。

日志解析方法：FT-tree

3. 半结构化的alerts被转化为正规化的模板，从模板中抽取一些结构特征（将经常组合出现的单词聚到一起）

主题学习方法：Biterm Topic Model

4. 提高稀有词的权重

方法：IDF（Inverse Document Frequency，逆文档频率）

5. 时间特征

1）报警频率【反比】

2）报警周期【反比】

检测方法：Autocorrelation Function (ACF)

3）报警计数：在一个短的时间窗口中爆发的大量alert更倾向于严重。

4）故障时间间隔：突发意味严重

6. 其他特征 

故障类型（用程序、操作系统、网络、内存、中间件和其他）和故障发生时间（繁忙时间：9:00-11:00 和 14:00-16:00、白天或晚上、工作日或周末）

* KPI特征抽取

1. 选择具有代表性的 business KPI（响应时间、成功率、事务量、处理时间）和 server KPI（CPU利用率、I/O等待、内存利用率、负载、网络包数、进程数、磁盘I/O）

2. 方法：基于LSTM的多变量时间序列异常检测算法

3. 使用预测误差来衡量异常程度。模型中，使用整体的预测误差和在每一个维度上的误差作为特征，可以发现误差越高，严重程度评分越高。【大概因为预测误差是反映了数据的突变，进而体现故障的严重（罕见吧】

* 排序模型

采用基于回归树的XGBoost逐点排序算法

## 2、Diagnosing Root Causes of Intermittent Slow Queries in Cloud Databases

思路：在云数据库中出现了数据库本身和计算机级别的性能问题，导致服务之间读取数据库过程出现了间歇性慢查询，提出了基于机器学习算法的间歇性异常诊断器。
其中，该框架有两个阶段：离线分析和在线诊断、更新，又包含4个模块：异常提取、依赖清理、面向类型聚类、贝叶斯模型。

数据是 多维KPI值

* 异常提取

解决问题：kpi异常多样、异常表现复杂

方法：指定时间段，收集规定时间段内的KPI片段，从其中提取异常类型。其中在识别尖峰的时候，使用了一种鲁棒阈值。

* 依赖清理

解决问题：kpi异常相关

方法：计算关联规则置信度

* 面向类型聚类

解决问题：数据多、标注难、4个基本kpi异常表现类型、kpi类型相关而根因相关

方法：TOPIC算法

* 贝叶斯模型

解决问题：可解释化

## 3、Localizing Failure Root Causes in a Microservice through Causality Inference

思路：提出一个并行模型，MicoCause，目标是在微服务中定位故障根因的度量。模型有两个主要部分，一个基于改进的PC算法学习服务之间关系的因果图构造，另一个是面向时间因果图的根本原因诊断的随机游走算法。
基于经验故障传播时间，将故障前4小时到该时刻的故障微服务监测指标作为MicroCause的输入。

数据集是 故障记录单ticket（ three key elements ） + KPI + metric 

* 概念

1⃣️ KPI 和 metric

两个都属于时间序列型指标数据，两种指标均存在时序间、跨类别的异常延迟传播的性质。

KPI：关键绩效指标，可以体现用户感知指标，可反应出服务质量的表象，因此出现异常时，都要晚于metric

metric：度量，一般表示微服务底层组件的状态，出现出问题的时间要早，因此一般是故障根因

2⃣️ 故障 和 异常

微服务故障是指微服务不起作用，严重影响用户体验质量的事件，微服务的异常就是当它的行为偏离正常情况。

可能导致微服务故障的组件主要有三种：上游组件、下游组件、部署环境。

当一个KPI异常时，它通常表示微服务故障，这是由微服务的上游组件、下游组件或部署环境的故障引起的。

微服务故障的根本原因可以用异常度量来表示。

当微服务的某个KPI异常时，该微服务被视为存在故障。异常度量可能是微服务故障的异常KPI的潜在根本原因。然而，度量的异常并不一定会导致 KPI 异常。因为存在平台可接入负载均衡策略等机制。

* 两个问题

	基于独立、同分布的因果图不能捕捉 KPI 和 metric 延迟传播

忽略序列模式的因果图可能无法准确地学习因果关系。时间序列之间的因果关系具有时滞性。

	基于相关性的随机游走可能无法准确定位根本原因

KPI 和 metric是异构的。通常情况下，同一类别的监测指标比不同类别的监测指标相关性更大。

如：异常KPI，Web RT 与消费者中间 RT 的关系更密切，但它与根因 度量（即Web QPS）无关。

* 方法

1⃣️ 故障因果图——PCTS算法

 输入是N个独立的同分布样本（KPI 和 metric）每个样本包含M个值，分别代表M个随机变量的观测值。
 
 输出一个有M个节点的有向无环图G，其中每个节点代表一个随机变量（表示监测指标一个时间点的值）。因此，这个图还不能用来定位根本原因。主要是需要图，其每个节点代表一个指标。
 
2⃣️ 故障检测——SPOT + TCORW算法

度量值异常的发现：SPOT通过极值理论来检测时间序列的突变。异常检测模块完成后，输出各指标的异常时间和异常程度。

故障的诊断与根因排名：基于时间的随机游走模型

## 4、两篇硕士论文

看文章结构，找写作套路

# 思路：

多维度监测数据，发现问题，对实现LB起作用，找自己的规则，使用数理统计或深度学习的方法。

【特征工程】特征提取、融合（特征来自多维metric，同时感觉一定要考虑时间序列特性）-> 【问题识别】基于机器学习特征，识别、自动发现负载不均衡点（这里可以考虑将问题分类) -> 【治理优化】治理不均衡（不同类型的问题采取不同的治理方法：熔断、流量转发、扩容、调度）

